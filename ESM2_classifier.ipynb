{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "import logging\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from accelerate import Accelerator\n",
    "from datasets import Dataset\n",
    "from Bio import SeqIO\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path=\"configs/train.yaml\"):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "def load_data_from_folder(folder_path):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    # Get all fasta files (support .fasta and .faa)\n",
    "    folder = Path(folder_path)\n",
    "    fasta_files = list(folder.glob(\"*.fasta\")) + list(folder.glob(\"*.faa\"))\n",
    "\n",
    "    if not fasta_files:\n",
    "        logger.warning(f\"No fasta/faa files found in {folder_path}\")\n",
    "\n",
    "    for file_path in fasta_files:\n",
    "        # Extract label from filename (e.g., \"Cas12.fasta\" -> \"Cas12\")\n",
    "        label = file_path.stem\n",
    "\n",
    "        # Read sequences\n",
    "        try:\n",
    "            records = list(SeqIO.parse(file_path, \"fasta\"))\n",
    "            if not records:\n",
    "                logger.warning(f\"No sequences found in {file_path}\")\n",
    "                continue\n",
    "            for record in records:\n",
    "                sequences.append(str(record.seq))\n",
    "                labels.append(label)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    return sequences, labels\n",
    "\n",
    "def create_dataset(folder_path, label_to_id=None):\n",
    "    if not Path(folder_path).exists():\n",
    "        logger.warning(f\"Directory {folder_path} does not exist.\")\n",
    "        return None, None\n",
    "\n",
    "    sequences, labels_text = load_data_from_folder(folder_path)\n",
    "\n",
    "    if not sequences:\n",
    "        logger.warning(f\"No sequences collected from {folder_path}\")\n",
    "        return None, None\n",
    "\n",
    "    if label_to_id is None:\n",
    "        unique_labels = sorted(list(set(labels_text)))\n",
    "        label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    # Filter labels\n",
    "    labels_id = []\n",
    "    filtered_sequences = []\n",
    "    for seq, label in zip(sequences, labels_text):\n",
    "        if label in label_to_id:\n",
    "            labels_id.append(label_to_id[label])\n",
    "            filtered_sequences.append(seq)\n",
    "        else:\n",
    "            logger.warning(f\"Label {label} not in label map. Skipping sequence.\")\n",
    "\n",
    "    from datasets import Dataset # Import here to ensure visibility if needed or rely on outer scope\n",
    "    # Changed \"label\" to \"labels\" for HF model compatibility\n",
    "    dataset = Dataset.from_dict({\"sequence\": filtered_sequences, \"labels\": labels_id})\n",
    "    return dataset, label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate args\n",
    "config_file = \"configs/train.yaml\"\n",
    "\n",
    "# 1. Initialize Accelerator\n",
    "accelerator = Accelerator()\n",
    "print(\"Accelerator initialized\")\n",
    "\n",
    "# 2. Load Config\n",
    "config = load_config(config_file)\n",
    "print(f\"Loaded config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Data & Create Label Map\n",
    "print(\"Loading training data...\")\n",
    "train_dataset, label_to_id = create_dataset(\"data/train\")\n",
    "if train_dataset is None:\n",
    "    raise ValueError(\"Training data is empty! Please populate data/train.\")\n",
    "\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "num_labels = len(label_to_id)\n",
    "print(f\"Found {num_labels} classes: {label_to_id}\")\n",
    "\n",
    "print(\"Loading validation and test data...\")\n",
    "val_path = \"data/validation\" if Path(\"data/validation\").exists() else \"data/validate\"\n",
    "val_dataset, _ = create_dataset(val_path, label_to_id)\n",
    "test_dataset, _ = create_dataset(\"data/test\", label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tokenization\n",
    "model_name = config[\"model_name\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"sequence\"], padding=\"max_length\", truncation=True, max_length=config[\"max_length\"])\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    train_dataset_tokenized = train_dataset.map(tokenize_batch, batched=True, remove_columns=[\"sequence\"])\n",
    "    val_dataset_tokenized = val_dataset.map(tokenize_batch, batched=True, remove_columns=[\"sequence\"]) if val_dataset else None\n",
    "    test_dataset_tokenized = test_dataset.map(tokenize_batch, batched=True, remove_columns=[\"sequence\"]) if test_dataset else None\n",
    "\n",
    "train_dataset_tokenized.set_format(\"torch\")\n",
    "if val_dataset_tokenized: val_dataset_tokenized.set_format(\"torch\")\n",
    "if test_dataset_tokenized: test_dataset_tokenized.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PKri",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. DataLoaders\n",
    "batch_size = config[\"batch_size\"]\n",
    "train_dataloader = DataLoader(train_dataset_tokenized, shuffle=True, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset_tokenized, batch_size=batch_size) if val_dataset_tokenized else None\n",
    "test_dataloader = DataLoader(test_dataset_tokenized, batch_size=batch_size) if test_dataset_tokenized else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model\n",
    "print(f\"Loading model {model_name}...\")\n",
    "_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels,\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "# 7. LoRA\n",
    "print(\"Applying LoRA...\")\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, \n",
    "    inference_mode=False, \n",
    "    r=config[\"lora_r\"], \n",
    "    lora_alpha=config[\"lora_alpha\"], \n",
    "    lora_dropout=config[\"lora_dropout\"],\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"dense\"] \n",
    ")\n",
    "model_lora = get_peft_model(_model, peft_config)\n",
    "if accelerator.is_local_main_process:\n",
    "    model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Optimizer & Scheduler\n",
    "optimizer = torch.optim.AdamW(model_lora.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Prepare\n",
    "# Use new variable names to avoid marimo global variable redefinition conflict\n",
    "p_model, p_optimizer, p_train_dataloader, p_val_dataloader, p_lr_scheduler = accelerator.prepare(\n",
    "    model_lora, optimizer, train_dataloader, val_dataloader, lr_scheduler\n",
    ")\n",
    "if test_dataloader:\n",
    "    p_test_dataloader = accelerator.prepare(test_dataloader)\n",
    "else:\n",
    "    p_test_dataloader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Training Loop\n",
    "def run_training_loop():\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        p_model.train()\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(p_train_dataloader):\n",
    "            if i == 0:\n",
    "                # Debug print\n",
    "                if 'labels' not in batch:\n",
    "                     print(f\"WARNING: 'labels' key missing in batch! Keys: {list(batch.keys())}\")\n",
    "\n",
    "            outputs = p_model(**batch)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            if loss is None:\n",
    "                 raise ValueError(f\"Model return None loss. Batch keys: {list(batch.keys())}\")\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            p_optimizer.step()\n",
    "            p_lr_scheduler.step()\n",
    "            p_optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(p_train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        if p_val_dataloader:\n",
    "            p_model.eval()\n",
    "            val_loss = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            for batch in p_val_dataloader:\n",
    "                with torch.no_grad():\n",
    "                    outputs = p_model(**batch)\n",
    "                    val_loss += outputs.loss.item()\n",
    "                    predictions = outputs.logits.argmax(dim=-1)\n",
    "                    # Changed \"label\" to \"labels\"\n",
    "                    preds, labels = accelerator.gather_for_metrics((predictions, batch[\"labels\"]))\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            avg_val_loss = val_loss / len(p_val_dataloader)\n",
    "            accuracy = accuracy_score(all_labels, all_preds)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - Val Loss: {avg_val_loss:.4f} - Val Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "run_training_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Final Evaluation on Test Set\n",
    "def run_test_loop():\n",
    "    if p_test_dataloader:\n",
    "        print(\"Evaluating on test set...\")\n",
    "        p_model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for batch in p_test_dataloader:\n",
    "            with torch.no_grad():\n",
    "                outputs = p_model(**batch)\n",
    "                predictions = outputs.logits.argmax(dim=-1)\n",
    "                # Changed \"label\" to \"labels\"\n",
    "                preds, labels = accelerator.gather_for_metrics((predictions, batch[\"labels\"]))\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f\"Test Results: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "run_test_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Save Model\n",
    "output_dir = config[\"output_dir\"]\n",
    "if output_dir:\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(p_model)\n",
    "    unwrapped_model.save_pretrained(output_dir, is_main_process=accelerator.is_main_process)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    if accelerator.is_main_process:\n",
    "        # Save label map\n",
    "        with open(Path(output_dir) / \"label_map.yaml\", \"w\") as f:\n",
    "            yaml.dump(label_to_id, f)\n",
    "        print(f\"Model saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
