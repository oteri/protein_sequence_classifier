{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oteri/protein_sequence_classifier/blob/main/ESM2_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/oteri/protein_sequence_classifier.git"
      ],
      "metadata": {
        "id": "rU45xkMNibY3"
      },
      "id": "rU45xkMNibY3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd protein_sequence_classifier/ && uv pip install . && make download_all_datasets"
      ],
      "metadata": {
        "id": "_OxdeghMi7Kv"
      },
      "id": "_OxdeghMi7Kv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hbol",
      "metadata": {
        "id": "Hbol"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import yaml\n",
        "import torch\n",
        "import logging\n",
        "import argparse\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from accelerate import Accelerator\n",
        "from datasets import Dataset\n",
        "from Bio import SeqIO\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    force=True,   # <-- Forcing reconfiguration so to be active even in colab\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MJUe",
      "metadata": {
        "id": "MJUe"
      },
      "outputs": [],
      "source": [
        "def load_config(config_path=\"configs/train.yaml\"):\n",
        "    with open(config_path, \"r\") as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    return config\n",
        "\n",
        "def load_data_from_folder(folder_path):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    # Get all fasta files (support .fasta and .faa)\n",
        "    folder = Path(folder_path)\n",
        "    fasta_files = list(folder.glob(\"*.fasta\")) + list(folder.glob(\"*.faa\"))\n",
        "\n",
        "    if not fasta_files:\n",
        "        raise ValueError(f\"No fasta/faa files found in {folder_path}\")\n",
        "\n",
        "    for file_path in fasta_files:\n",
        "        # Extract label from filename (e.g., \"Cas12.fasta\" -> \"Cas12\")\n",
        "        label = file_path.stem\n",
        "\n",
        "        # Read sequences\n",
        "        try:\n",
        "            records = list(SeqIO.parse(file_path, \"fasta\"))\n",
        "            if not records:\n",
        "                logger.warning(f\"No sequences found in {file_path}\")\n",
        "                continue\n",
        "            for record in records:\n",
        "                sequences.append(str(record.seq))\n",
        "                labels.append(label)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading {file_path}: {e}\")\n",
        "\n",
        "    return sequences, labels\n",
        "\n",
        "def create_dataset(folder_path, label_to_id=None):\n",
        "    if not Path(folder_path).exists():\n",
        "        raise ValueError(f\"Directory {folder_path} does not exist.\")\n",
        "\n",
        "    sequences, labels_text = load_data_from_folder(folder_path)\n",
        "\n",
        "    if not sequences:\n",
        "        raise ValueError(f\"No sequences collected from {folder_path}\")\n",
        "\n",
        "    if label_to_id is None:\n",
        "        unique_labels = sorted(list(set(labels_text)))\n",
        "        label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "    # Filter labels\n",
        "    labels_id = []\n",
        "    filtered_sequences = []\n",
        "    for seq, label in zip(sequences, labels_text):\n",
        "        if label in label_to_id:\n",
        "            labels_id.append(label_to_id[label])\n",
        "            filtered_sequences.append(seq)\n",
        "        else:\n",
        "            logger.warning(f\"Label {label} not in label map. Skipping sequence.\")\n",
        "\n",
        "    from datasets import Dataset # Import here to ensure visibility if needed or rely on outer scope\n",
        "    # Changed \"label\" to \"labels\" for HF model compatibility\n",
        "    dataset = Dataset.from_dict({\"sequence\": filtered_sequences, \"labels\": labels_id})\n",
        "    return dataset, label_to_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vblA",
      "metadata": {
        "id": "vblA"
      },
      "outputs": [],
      "source": [
        "# Simulate args\n",
        "config_file = \"protein_sequence_classifier/configs/train.yaml\"\n",
        "\n",
        "# 1. Initialize Accelerator\n",
        "accelerator = Accelerator()\n",
        "logger.info(\"Accelerator initialized\")\n",
        "\n",
        "# 2. Load Config\n",
        "config = load_config(config_file)\n",
        "logger.info(f\"Loaded config: {config}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bkHC",
      "metadata": {
        "id": "bkHC"
      },
      "outputs": [],
      "source": [
        "# 3. Load Data & Create Label Map\n",
        "logger.info(\"Loading training data...\")\n",
        "train_path = \"protein_sequence_classifier/data/train\"\n",
        "val_path  =  \"protein_sequence_classifier/data/validate\"\n",
        "test_path  = \"protein_sequence_classifier/data/test\"\n",
        "\n",
        "try:\n",
        "  train_dataset, label_to_id = create_dataset()\n",
        "except ValueError:\n",
        "    raise ValueError(f\"Training data is empty! Please populate {train_path}.\")\n",
        "\n",
        "id_to_label = {v: k for k, v in label_to_id.items()}\n",
        "num_labels = len(label_to_id)\n",
        "logger.info(f\"Found {num_labels} classes: {label_to_id}\")\n",
        "\n",
        "\n",
        "logger.info(f\"Loading validation and test data from {val_path} and {test_path}\")\n",
        "\n",
        "try:\n",
        "  val_dataset, _ = create_dataset(val_path, label_to_id)\n",
        "except ValueError:\n",
        "  raise ValueError(f\"Validation data is empty! Please populate {val_path}.\")\n",
        "\n",
        "try:\n",
        "  test_dataset, _ = create_dataset(test_path, label_to_id)\n",
        "except ValueError:\n",
        "  raise ValueError(f\"Test data is empty! Please populate {test_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lEQa",
      "metadata": {
        "id": "lEQa"
      },
      "outputs": [],
      "source": [
        "# 4. Tokenization\n",
        "model_name = config[\"model_name\"]\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return tokenizer(batch[\"sequence\"], padding=\"max_length\", truncation=True, max_length=config[\"max_length\"])\n",
        "\n",
        "with accelerator.main_process_first():\n",
        "    train_dataset_tokenized = train_dataset.map(tokenize_batch, batched=True, remove_columns=[\"sequence\"])\n",
        "    val_dataset_tokenized = val_dataset.map(tokenize_batch, batched=True, remove_columns=[\"sequence\"])\n",
        "    test_dataset_tokenized = test_dataset.map(tokenize_batch, batched=True, remove_columns=[\"sequence\"])\n",
        "\n",
        "train_dataset_tokenized.set_format(\"torch\")\n",
        "val_dataset_tokenized.set_format(\"torch\")\n",
        "test_dataset_tokenized.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PKri",
      "metadata": {
        "id": "PKri"
      },
      "outputs": [],
      "source": [
        "# 5. DataLoaders\n",
        "batch_size = config[\"batch_size\"]\n",
        "train_dataloader = DataLoader(train_dataset_tokenized, shuffle=True, batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_dataset_tokenized, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_dataset_tokenized, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xref",
      "metadata": {
        "id": "Xref"
      },
      "outputs": [],
      "source": [
        "# 6. Model\n",
        "logger.info(f\"Loading model {model_name}...\")\n",
        "_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id_to_label,\n",
        "    label2id=label_to_id\n",
        ")\n",
        "\n",
        "# 7. LoRA\n",
        "logger.info(\"Applying LoRA...\")\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    inference_mode=False,\n",
        "    r=config[\"lora_r\"],\n",
        "    lora_alpha=config[\"lora_alpha\"],\n",
        "    lora_dropout=config[\"lora_dropout\"],\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"dense\"]\n",
        ")\n",
        "model_lora = get_peft_model(_model, peft_config)\n",
        "if accelerator.is_local_main_process:\n",
        "    model_lora.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SFPL",
      "metadata": {
        "id": "SFPL"
      },
      "outputs": [],
      "source": [
        "# 8. Optimizer & Scheduler\n",
        "optimizer = torch.optim.AdamW(model_lora.parameters(), lr=config[\"learning_rate\"])\n",
        "\n",
        "num_epochs = config[\"num_epochs\"]\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BYtC",
      "metadata": {
        "id": "BYtC"
      },
      "outputs": [],
      "source": [
        "# 9. Prepare\n",
        "# Use new variable names to avoid marimo global variable redefinition conflict\n",
        "p_model, p_optimizer, p_train_dataloader, p_val_dataloader, p_lr_scheduler = accelerator.prepare(\n",
        "    model_lora, optimizer, train_dataloader, val_dataloader, lr_scheduler\n",
        ")\n",
        "if test_dataloader:\n",
        "    p_test_dataloader = accelerator.prepare(test_dataloader)\n",
        "else:\n",
        "    p_test_dataloader = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RGSE",
      "metadata": {
        "id": "RGSE"
      },
      "outputs": [],
      "source": [
        "# 10. Training Loop\n",
        "def run_training_loop():\n",
        "    logger.info(\"Starting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        p_model.train()\n",
        "        total_loss = 0\n",
        "        for i, batch in enumerate(p_train_dataloader):\n",
        "            if i == 0:\n",
        "                # Debug print\n",
        "                if 'labels' not in batch:\n",
        "                     logger.warn(f\"WARNING: 'labels' key missing in batch! Keys: {list(batch.keys())}\")\n",
        "\n",
        "            outputs = p_model(**batch)\n",
        "\n",
        "            loss = outputs.loss\n",
        "            if loss is None:\n",
        "                 raise ValueError(f\"Model return None loss. Batch keys: {list(batch.keys())}\")\n",
        "\n",
        "            accelerator.backward(loss)\n",
        "            p_optimizer.step()\n",
        "            p_lr_scheduler.step()\n",
        "            p_optimizer.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(p_train_dataloader)\n",
        "        logger.info(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "\n",
        "        # Validation\n",
        "        if p_val_dataloader:\n",
        "            p_model.eval()\n",
        "            val_loss = 0\n",
        "            all_preds = []\n",
        "            all_labels = []\n",
        "            for batch in p_val_dataloader:\n",
        "                with torch.no_grad():\n",
        "                    outputs = p_model(**batch)\n",
        "                    val_loss += outputs.loss.item()\n",
        "                    predictions = outputs.logits.argmax(dim=-1)\n",
        "                    # Changed \"label\" to \"labels\"\n",
        "                    preds, labels = accelerator.gather_for_metrics((predictions, batch[\"labels\"]))\n",
        "                    all_preds.extend(preds.cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            avg_val_loss = val_loss / len(p_val_dataloader)\n",
        "            accuracy = accuracy_score(all_labels, all_preds)\n",
        "            logger.info(f\"Epoch {epoch+1}/{num_epochs} - Val Loss: {avg_val_loss:.4f} - Val Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "run_training_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Kclp",
      "metadata": {
        "id": "Kclp"
      },
      "outputs": [],
      "source": [
        "# 11. Final Evaluation on Test Set\n",
        "def run_test_loop():\n",
        "    if p_test_dataloader:\n",
        "        logger.info(\"Evaluating on test set...\")\n",
        "        p_model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for batch in p_test_dataloader:\n",
        "            with torch.no_grad():\n",
        "                outputs = p_model(**batch)\n",
        "                predictions = outputs.logits.argmax(dim=-1)\n",
        "                # Changed \"label\" to \"labels\"\n",
        "                preds, labels = accelerator.gather_for_metrics((predictions, batch[\"labels\"]))\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "        logger.info(f\"Test Results: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "run_test_loop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "emfo",
      "metadata": {
        "id": "emfo"
      },
      "outputs": [],
      "source": [
        "# 12. Save Model\n",
        "output_dir = config[\"output_dir\"]\n",
        "if output_dir:\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(p_model)\n",
        "    unwrapped_model.save_pretrained(output_dir, is_main_process=accelerator.is_main_process)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    if accelerator.is_main_process:\n",
        "        # Save label map\n",
        "        with open(Path(output_dir) / \"label_map.yaml\", \"w\") as f:\n",
        "            yaml.dump(label_to_id, f)\n",
        "        logger.info(f\"Model saved to {output_dir}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}